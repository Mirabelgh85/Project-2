# Project-2
This project seeks to explore different ways of gathering data for data analysis and also to concentrate on the data wrangling stage of data analysis.
The data wrangling process includes gathering data, assessing data and cleaning of data.
In this project, three pieces of data were gathered using three different methods during the gathering stage. 
The first piece of data, 'twitter-archive-enhanced.csv' was obtained by directly downloading and uploading into the jupyter notebook to be read using pandas. 
The second piece of data was obtained by using the requests library to download a file named 'image-predictions.tsv'. The third file was obtained by using the tweepy library to query additional data via the Twitter API. A json file named 'tweet_json.txt was obtained from this process. 
The next stage is the assessing stage. At this stage, two types of assessment were carried out. These are visual and programmatic types.
After assessment, eight quality issues and two tidiness issues were arrived at. 
The eight quality issues included df1: missing retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp values, df1:Timestamp should be of data type date-time, tweet_id should be of data type string, in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id should be integers. Tweet_id should have string data type. API_df: date_created should be of data type date-time, df1: Invalid dog names, df1: None in doggo, floofer, pupper and puppo columns is treated as a non-null value, df2: non-descriptive column headers p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog, df1: html tags in the source column makes it hard to read, df1: Not all the data are dog ratings, some are retweets and Retweet columns need to be removed as they are not needed. 
The two tidiness issues included df1: dog names doggo, floofer, pupper and puppo should be in one column and all dataframes should be merged into one master dataframe.
The final stage of the wrangling process is the cleaning stage. This is where all the issues documented while assessing the three pieces of data. Copies of the pieces of data were made before cleaning. Cleaning involves defining coding and testing. Defining deals with briefly describing how the issue will be tackled while coding and testing involve writing codes to tackle the issue and then making sure that issue has been solved before moving on to tackle the next issue. 
After cleaning, all the three pieces of data were merged into one master dataframe. Dataframe was then stored as a csv file called 'twitter_archive_master.csv' and then analysis was done on this dataframe.
